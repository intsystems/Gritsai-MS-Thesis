import nltk
import numpy as np
import random
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_distances
from nltk.corpus import wordnet
from scipy.special import rel_entr 
from nltk.tokenize import sent_tokenize

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

def get_synonym(word):
    """
    Retrieves a synonym for the given word using the WordNet lexical database.
    
    Parameters:
    word (str): The word for which to find a synonym.
    
    Returns:
    str: A synonym of the input word if available, otherwise the original word.
    """
    synonyms = wordnet.synsets(word)
    if synonyms:
        return synonyms[0].lemmas()[0].name() if synonyms[0].lemmas()[0].name() != word else word
    return word

def perturb_text(text, sample_ratio=0.7):
    """
    Modifies the input text by replacing a subset of words with their synonyms.

    Parameters:
    text (str): The input text to be perturbed.
    sample_ratio (float): The probability (between 0 and 1) that each word 
                          will be replaced by a synonym. Default is 0.7.

    Returns:
    str: The perturbed text with some words replaced by their synonyms.
    """
    words = text.split()
    perturbed_words = [get_synonym(word) if random.random() < sample_ratio else word for word in words]
    return ' '.join(perturbed_words)

def change_texts_local(texts):
    """
    Randomly shuffles the order of sentences in each text from a list of texts.

    Parameters:
    texts (list of str): A list of texts (strings), where each text is a block 
                         of sentences to be shuffled.

    Returns:
    list of str: A list containing the modified versions of the input texts, 
                 where the sentences in each text have been randomly reordered.

    """
    changed_order_sentences = []
    for item in texts:
        sentences = sent_tokenize(item)
        poses = [(item.find(sent), item.find(sent) + len(sent)) for sent in sentences]
        random.shuffle(poses)
        text_fragments = [item[b:e] for b, e in poses]
        changed_order_sentences.append(' '.join(text_fragments))
    return changed_order_sentences

def adversarial_token_perturbation(human_texts, ai_texts, model):
    """
    Computes the log difference in embedding shifts between AI-generated texts and human-generated 
    texts after applying adversarial perturbations to the tokens.

    Parameters:
    human_texts (list of str): A list of texts generated by humans.
    ai_texts (list of str): A list of texts generated by AI.
    model (object): A language model capable of encoding texts into embeddings.

    Returns:
    float: The absolute difference between the logs of the average embedding shifts for 
           AI-generated texts and human-generated texts after adversarial perturbation.

    """
    perturbed_ai_texts = [perturb_text(text) for text in ai_texts]
    perturbed_human_texts = [perturb_text(text) for text in human_texts]

    original_embeddings_human = model.encode(human_texts)
    original_embeddings_ai = model.encode(ai_texts)
    perturbed_ai_embeddings = model.encode(perturbed_ai_texts)
    perturbed_human_embeddings = model.encode(perturbed_human_texts)

    ai_shifts = cosine_distances(original_embeddings_ai, perturbed_ai_embeddings).diagonal()
    human_shifts = cosine_distances(original_embeddings_human, perturbed_human_embeddings).diagonal()

    avg_ai_shift = np.mean(ai_shifts)
    avg_human_shift = np.mean(human_shifts)

    delta_shift = np.abs(np.log(avg_ai_shift) - np.log(avg_human_shift))

    return delta_shift


def sentence_shuffling(human_texts, ai_texts, model, epsilon=1e-10):
    """
    Computes the Kullback-Leibler divergence between the sentence-level perturbation distributions 
    for AI-generated texts and human-generated texts.

    Parameters:
    human_texts (list of str): A list of texts generated by humans.
    ai_texts (list of str): A list of texts generated by AI.
    model (object): A language model capable of encoding texts into embeddings.
    epsilon (float): Value added to avoid division by zero. Default is 1e-10.

    Returns:
    float: The KL divergence between the normalized distributions of sentence-level perturbation shifts 
           for AI-generated texts and human-generated texts.

    """
    perturbed_ai_texts = change_texts_local(ai_texts)
    perturbed_human_texts = change_texts_local(human_texts)

    original_embeddings_human = model.encode(human_texts)
    original_embeddings_ai = model.encode(ai_texts)
    perturbed_ai_embeddings = model.encode(perturbed_ai_texts)
    perturbed_human_embeddings = model.encode(perturbed_human_texts)

    ai_shifts = cosine_distances(original_embeddings_ai, perturbed_ai_embeddings).diagonal()
    human_shifts = cosine_distances(original_embeddings_human, perturbed_human_embeddings).diagonal()

    ai_shift_distribution = (ai_shifts + epsilon) / np.sum(ai_shifts + epsilon)
    human_shift_distribution = (human_shifts + epsilon) / np.sum(human_shifts + epsilon)

    kl_divergence_ss = np.sum(rel_entr(human_shift_distribution, ai_shift_distribution))

    return kl_divergence_ss

def main():
    ai_texts = # your path to list with AI-generated texts
    human_texts = # your path to list with human-written texts

    model = SentenceTransformer("intfloat/multilingual-e5-large")

    delta_shift = adversarial_token_perturbation(human_texts, ai_texts, model)
    kl_divergence_shuffling = sentence_shuffling(human_texts, ai_texts, model)

    print("Adversarial Token Perturbation shift score: ", delta_shift)
    print("Sentence Shuffling KL-divergence score: ", kl_divergence_shuffling)

if __name__ == '__main__':
    main()
